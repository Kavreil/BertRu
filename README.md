## Описание проекта

Проект реализует задачу классификации текстов с использованием предобученной модели RuBERT (rubert-tiny) . Модель обучается на датасете, содержащем тексты новостей и их метки категорий (например, "Россия", "Мир", "Экономика"). Проект демонстрирует использование библиотек Transformers , PyTorch и Scikit-learn для токенизации текстов, обучения модели и оценки результатов.
* * *
Анализ данных :
Построена гистограмма распределения категорий для анализа баланса классов.
Вычислена максимальная длина токенизированных последовательностей для определения параметра max_len.
* * *
Токенизация текстов :
Используется токенизатор из библиотеки Transformers для преобразования текстов в тензоры.
Тексты обрезаются до максимальной длины 512 токенов.
* * *
Построение модели :
Реализован классификатор на основе предобученной модели RuBERT с добавлением слоев Dropout и Linear для классификации текстов.
Модель компилируется с функцией потерь CrossEntropyLoss и оптимизатором Adam.
* * *
Обучение модели :
Модель обучается на 5 эпохах с использованием обучающего датасета.
На каждой эпохе выводятся значения потерь и точности для обучающей и проверочной выборок.
Оценка результатов :
Строится матрица ошибок для визуализации качества классификации.
Выводится отчет о классификации с метриками Precision, Recall и F1-score для каждого класса.
* * *
Тестирование модели :
Модель тестируется на тестовом датасете, и выводятся примеры предсказаний вместе с истинными метками.
